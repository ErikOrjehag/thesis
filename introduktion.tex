\chapter{Introduction}\label{cha:intro}

This thesis aims to show how new neural network techniques can be used to predict depth and relative camera motion for an image sequence captured by a monocular \abbrRGB camera. Imagine closing one eye and looking out into the world, it is trivial as a human to detect motion and estimate how the head moves in relation to what is seen. Calculating camera movement from an image sequence is a well studied problem and is usually done by finding corresponding features in the images and calculate (using projective geometry) which camera movement can give rise to such correspondences and their relative movement between frames in a sequence.

Recent research has shown that its possible to predict depth and relative motion from a sequence of images taken with a monocular \abbrRGB camera. The training data is a sequence of unlabeled images with a small relative motion, for example looking out from the front window of a moving car. Given a target view and a new nearby view its possible to train a depth predicting and pose predicting \abbrCNN jointly using a combined loss function. The depth and pose predictions are used to warp nearby views to the target view and the loss is based on the similarity achieved after warping.

\section{Contributions}

A performance comparison of different techniques described in current research papers. Training and testing on new datasets.

\section{Motivation}

Localization by only using visual input is highly desirable in robotics applications due to the low hardware cost and power consumption of using cameras compared to, for example, 3D lidars. Obtaining labeled data can be a tedious task, that's why this thesis will focus on unsupervised learning on unlabeled data.

\section{Research Questions}

\begin{enumerate}
	
	\item What ideas from previous work can be combined and what is the performance gains if any?
	\item How well does previous methods work on new datasets not tested in the original papers? In what ways do the results break down?
	\item What alteration to previous loss functions can be made to improve results?
	\item How does the amount of training data affect the results? Can the performance of the methods from previous research be improved simply by training on more data?
	\item Can the results from the original papers be replicated in the PyTorch framework?
	
\end{enumerate}

\section{Delimitations}

The visual localization problem can be solved using, for example, a stereoscopic camera or a time of flight camera. But this thesis will only explore the use of a monoscopic, non depth sensing, \abbrRGB camera, because it enables applications where hardware cost is a big factor.

In a full \abbrSFM pipeline both 3D-reconstruction, bundle adjustment and loop-closure detection is usually done as well. This will not be part of this thesis project.