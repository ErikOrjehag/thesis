\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{kitti}
\citation{lyft2019}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Method}{9}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{cha:method}{{4}{9}{Method}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Unsupervised depth and ego motion learning}{9}{section.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Sequence datasets}{9}{subsection.4.1.1}}
\citation{sfmlearner}
\citation{dispnet}
\citation{resnet}
\citation{monodepth2}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces From top to bottom 3 frames from Kitti, $I_{t-1}$, $I_t$, $I_{t+1}$ with the sparse depth map overlayed on frame $I_t$.\relax }}{10}{figure.caption.12}}
\newlabel{fig:sequencedataset}{{4.1}{10}{From top to bottom 3 frames from Kitti, $I_{t-1}$, $I_t$, $I_{t+1}$ with the sparse depth map overlayed on frame $I_t$.\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Architectures for depth and ego motion CNNs}{10}{subsection.4.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Differentiable depth image warping}{10}{subsection.4.1.3}}
\newlabel{sec:diffwarp}{{4.1.3}{10}{Differentiable depth image warping}{subsection.4.1.3}{}}
\citation{spatialtransformernetworks}
\citation{ssim}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces The \textsc  {cnn}\xspace  predicts the depth map $D$ of the target image $I_t$, and also the relative movement, $T_{t\rightarrow t-1}, T_{t\rightarrow t+1}$ between the target image and the source images. Each pixel $p_t$ in the target image is projected onto a position in the source images which are sampled using bilinear interpolation. This should recreate the appearance of the target image but with pixels sampled from the source image. An appearance similarity metric between the original target image and the recreated target images can be used as the loss function for the \textsc  {cnn}\xspace  to learn to accurately predict correct depth and movement.\relax }}{12}{figure.caption.13}}
\newlabel{fig:warp}{{4.2}{12}{The \abbrCNN predicts the depth map $D$ of the target image $I_t$, and also the relative movement, $T_{t\rightarrow t-1}, T_{t\rightarrow t+1}$ between the target image and the source images. Each pixel $p_t$ in the target image is projected onto a position in the source images which are sampled using bilinear interpolation. This should recreate the appearance of the target image but with pixels sampled from the source image. An appearance similarity metric between the original target image and the recreated target images can be used as the loss function for the \abbrCNN to learn to accurately predict correct depth and movement.\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.4}Loss functions}{12}{subsection.4.1.4}}
\newlabel{sec:loss}{{4.1.4}{12}{Loss functions}{subsection.4.1.4}{}}
\citation{packnet}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.5}Handling occlusions}{13}{subsection.4.1.5}}
\newlabel{sec:occlusion}{{4.1.5}{13}{Handling occlusions}{subsection.4.1.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Disparity loss}{13}{section*.14}}
\citation{sfmlearner}
\citation{monodepth2}
\@writefile{toc}{\contentsline {paragraph}{Minimum loss across frames}{14}{section*.15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.6}Handling model limitations}{14}{subsection.4.1.6}}
\newlabel{sec:modellimit}{{4.1.6}{14}{Handling model limitations}{subsection.4.1.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Explainability mask}{14}{section*.16}}
\@writefile{toc}{\contentsline {paragraph}{Stationary pixels mask}{14}{section*.17}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Unsupervised keypoint learning}{14}{section.4.2}}
\citation{ransac}
\citation{consensus}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces \relax }}{15}{figure.caption.18}}
\newlabel{fig:unsuperpoint}{{4.3}{15}{\relax }{figure.caption.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Unsupervised consensus maximization}{15}{section.4.3}}
\citation{pointnet}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces \relax }}{17}{figure.caption.19}}
\newlabel{fig:consensus}{{4.4}{17}{\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Evaluation}{19}{section.4.4}}
\@setckpt{metod}{
\setcounter{page}{20}
\setcounter{equation}{0}
\setcounter{enumi}{5}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{rt@toplevel}{0}
\setcounter{part}{0}
\setcounter{chapter}{4}
\setcounter{section}{4}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{4}
\setcounter{table}{0}
\setcounter{parentequation}{0}
\setcounter{CPROT@family}{1}
\setcounter{CPROT@family@temp}{0}
\setcounter{CPROT@series}{2}
\setcounter{CPROT@series@temp}{0}
\setcounter{CPROT@shape}{14}
\setcounter{CPROT@shape@temp}{0}
\setcounter{CPROT@size}{60}
\setcounter{CPROT@size@temp}{0}
\setcounter{CPROT@temp@chars}{0}
\setcounter{float@type}{4}
\setcounter{NAT@ctr}{14}
\setcounter{ContinuedFloat}{0}
\setcounter{Item}{5}
\setcounter{bookmark@seq@number}{30}
\setcounter{lofdepth}{1}
\setcounter{lotdepth}{1}
\setcounter{KVtest}{1}
\setcounter{subfigure}{0}
\setcounter{subfigure@save}{0}
\setcounter{subtable}{0}
\setcounter{subtable@save}{0}
\setcounter{rtpapers@section}{0}
\setcounter{rtpapers@chapter}{0}
\setcounter{theorem}{0}
\setcounter{Example}{0}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{section@level}{1}
}
