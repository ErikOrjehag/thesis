\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{kitti}
\citation{lyft2019}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Method}{9}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{cha:method}{{4}{9}{Method}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Unsupervised depth and ego motion learning}{9}{section.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Sequence datasets}{9}{subsection.4.1.1}}
\citation{sfmlearner}
\citation{dispnet}
\citation{resnet}
\citation{monodepth2}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces From top to bottom 3 frames from Kitti, $I_{t-1}$, $I_t$, $I_{t+1}$ with the sparse depth map overlayed on frame $I_t$.\relax }}{10}{figure.caption.12}}
\newlabel{fig:sequencedataset}{{4.1}{10}{From top to bottom 3 frames from Kitti, $I_{t-1}$, $I_t$, $I_{t+1}$ with the sparse depth map overlayed on frame $I_t$.\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Architectures for depth and ego motion CNNs}{10}{subsection.4.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Differentiable depth image warping}{10}{subsection.4.1.3}}
\newlabel{sec:diffwarp}{{4.1.3}{10}{Differentiable depth image warping}{subsection.4.1.3}{}}
\citation{spatialtransformernetworks}
\citation{ssim}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces The \textsc  {cnn}\xspace  predicts the depth map $D$ of the target image $I_t$, and also the relative movement, $T_{t\rightarrow t-1}, T_{t\rightarrow t+1}$ between the target image and the source images. Each pixel $p_t$ in the target image is projected onto a position in the source images which are sampled using bilinear interpolation. This should recreate the appearance of the target image but with pixels sampled from the source image. An appearance similarity metric between the original target image and the recreated target images can be used as the loss function for the \textsc  {cnn}\xspace  to learn to accurately predict correct depth and movement.\relax }}{12}{figure.caption.13}}
\newlabel{fig:warp}{{4.2}{12}{The \abbrCNN predicts the depth map $D$ of the target image $I_t$, and also the relative movement, $T_{t\rightarrow t-1}, T_{t\rightarrow t+1}$ between the target image and the source images. Each pixel $p_t$ in the target image is projected onto a position in the source images which are sampled using bilinear interpolation. This should recreate the appearance of the target image but with pixels sampled from the source image. An appearance similarity metric between the original target image and the recreated target images can be used as the loss function for the \abbrCNN to learn to accurately predict correct depth and movement.\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.4}Loss functions}{12}{subsection.4.1.4}}
\newlabel{sec:loss}{{4.1.4}{12}{Loss functions}{subsection.4.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces The images illustrates the photometric reconstruction loss $ \mathcal  {L}_{ps} $ with $\alpha =0.85$ for each pixel in the reconstructed image. The left image shows the loss after 1 epoch of training and the right image shows the loss after 30 epochs. The reconstruction loss should decrease during training as the network learns to predict better depth maps, which is what we see.\relax }}{13}{figure.caption.14}}
\newlabel{fig:diff}{{4.3}{13}{The images illustrates the photometric reconstruction loss $ \mathcal {L}_{ps} $ with $\alpha =0.85$ for each pixel in the reconstructed image. The left image shows the loss after 1 epoch of training and the right image shows the loss after 30 epochs. The reconstruction loss should decrease during training as the network learns to predict better depth maps, which is what we see.\relax }{figure.caption.14}{}}
\citation{packnet}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces The top row of images illustrates the edge weighting terms $e^{-|\delta _y I_t|}$ and $e^{-|\delta _x I_t|}$ respectively. The weight is near 0 at the edges of tree trunks and near 1 on the pavement. This will preserve the details in the depth map around trees but keep the pavement smooth.\relax }}{14}{figure.caption.15}}
\newlabel{fig:edge}{{4.4}{14}{The top row of images illustrates the edge weighting terms $e^{-|\delta _y I_t|}$ and $e^{-|\delta _x I_t|}$ respectively. The weight is near 0 at the edges of tree trunks and near 1 on the pavement. This will preserve the details in the depth map around trees but keep the pavement smooth.\relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.5}Handling occlusions}{14}{subsection.4.1.5}}
\newlabel{sec:occlusion}{{4.1.5}{14}{Handling occlusions}{subsection.4.1.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Disparity loss}{14}{section*.16}}
\@writefile{toc}{\contentsline {paragraph}{Minimum loss across frames}{14}{section*.17}}
\citation{sfmlearner}
\citation{monodepth2}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.6}Handling model limitations}{15}{subsection.4.1.6}}
\newlabel{sec:modellimit}{{4.1.6}{15}{Handling model limitations}{subsection.4.1.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Explainability mask}{15}{section*.18}}
\@writefile{toc}{\contentsline {paragraph}{Stationary pixels mask}{15}{section*.19}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Unsupervised keypoint learning}{15}{section.4.2}}
\citation{ransac}
\citation{consensus}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces \relax }}{16}{figure.caption.20}}
\newlabel{fig:unsuperpoint}{{4.5}{16}{\relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Unsupervised consensus maximization}{16}{section.4.3}}
\citation{pointnet}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces \relax }}{19}{figure.caption.21}}
\newlabel{fig:consensus}{{4.6}{19}{\relax }{figure.caption.21}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Evaluation}{21}{section.4.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Depth}{21}{subsection.4.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Ego motion}{23}{subsection.4.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Keypoints}{23}{subsection.4.4.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.4}Consensus maximization}{23}{subsection.4.4.4}}
\@setckpt{metod}{
\setcounter{page}{24}
\setcounter{equation}{0}
\setcounter{enumi}{5}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{rt@toplevel}{0}
\setcounter{part}{0}
\setcounter{chapter}{4}
\setcounter{section}{4}
\setcounter{subsection}{4}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{6}
\setcounter{table}{0}
\setcounter{parentequation}{0}
\setcounter{CPROT@family}{1}
\setcounter{CPROT@family@temp}{0}
\setcounter{CPROT@series}{2}
\setcounter{CPROT@series@temp}{0}
\setcounter{CPROT@shape}{14}
\setcounter{CPROT@shape@temp}{0}
\setcounter{CPROT@size}{60}
\setcounter{CPROT@size@temp}{0}
\setcounter{CPROT@temp@chars}{0}
\setcounter{float@type}{4}
\setcounter{NAT@ctr}{14}
\setcounter{ContinuedFloat}{0}
\setcounter{Item}{5}
\setcounter{bookmark@seq@number}{34}
\setcounter{lofdepth}{1}
\setcounter{lotdepth}{1}
\setcounter{KVtest}{1}
\setcounter{subfigure}{0}
\setcounter{subfigure@save}{0}
\setcounter{subtable}{0}
\setcounter{subtable@save}{0}
\setcounter{rtpapers@section}{0}
\setcounter{rtpapers@chapter}{0}
\setcounter{theorem}{0}
\setcounter{Example}{0}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{section@level}{2}
}
