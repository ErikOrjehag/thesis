\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{sfmlearner}
\citation{dispnet}
\citation{monodepth2}
\citation{resnet}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Implementation}{19}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{cha:implementation}{{5}{19}{Implementation}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Depth and ego motion prediction}{19}{section.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Architectures for depth and ego motion CNNs}{19}{subsection.5.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces High level diagram of the network architectures used for depth and ego motion prediction. The layers from the depth encoder are concatenated into the layers of the decoder. Depth maps are computed at multiple scales in the decoder and are all used in the loss function. A separate network that takes as input 3 subsequent frames predicts the poses $\textbf  {T}_{t\rightarrow t-1}$ and $\textbf  {T}_{t\rightarrow t+1}$ between the target and nearby reference frames. The pose network shares encoder with the explain-ability mask predicting network (section \ref  {sec:modellimit}).\relax }}{20}{figure.caption.17}}
\newlabel{fig:net}{{5.1}{20}{High level diagram of the network architectures used for depth and ego motion prediction. The layers from the depth encoder are concatenated into the layers of the decoder. Depth maps are computed at multiple scales in the decoder and are all used in the loss function. A separate network that takes as input 3 subsequent frames predicts the poses $\textbf {T}_{t\rightarrow t-1}$ and $\textbf {T}_{t\rightarrow t+1}$ between the target and nearby reference frames. The pose network shares encoder with the explain-ability mask predicting network (section \ref {sec:modellimit}).\relax }{figure.caption.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Differentiable depth image warping}{20}{subsection.5.1.2}}
\newlabel{sec:diffwarp}{{5.1.2}{20}{Differentiable depth image warping}{subsection.5.1.2}{}}
\citation{spatialtransformernetworks}
\citation{ssim}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces The \textsc  {cnn}\xspace  predicts the depth map $\textbf  {D}$ of the target image $\textbf  {I}_t$, and also the relative movement, $\textbf  {T}_{t\rightarrow t-1}, \textbf  {T}_{t\rightarrow t+1}$ between the target image and the source images. Each pixel $p_t$ in the target image is projected onto a position in the source images which are sampled using bilinear interpolation. This should recreate the appearance of the target image but with pixels sampled from the source image. An appearance similarity metric between the original target image and the recreated target images can be used as the loss function for the \textsc  {cnn}\xspace  to learn to accurately predict correct depth and movement.\relax }}{21}{figure.caption.18}}
\newlabel{fig:warp}{{5.2}{21}{The \abbrCNN predicts the depth map $\textbf {D}$ of the target image $\textbf {I}_t$, and also the relative movement, $\textbf {T}_{t\rightarrow t-1}, \textbf {T}_{t\rightarrow t+1}$ between the target image and the source images. Each pixel $p_t$ in the target image is projected onto a position in the source images which are sampled using bilinear interpolation. This should recreate the appearance of the target image but with pixels sampled from the source image. An appearance similarity metric between the original target image and the recreated target images can be used as the loss function for the \abbrCNN to learn to accurately predict correct depth and movement.\relax }{figure.caption.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}Loss functions}{21}{subsection.5.1.3}}
\newlabel{sec:loss}{{5.1.3}{21}{Loss functions}{subsection.5.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces The images illustrates the photometric reconstruction loss $ \mathcal  {L}_{ps} $ for each pixel in a reconstructed image for the same frame but after different length of training. The left image shows the loss after 1 epoch of training and the right image shows the loss after 30 epochs. The reconstruction loss should decrease during training as the network learns to predict better depth maps, which is what we see.\relax }}{22}{figure.caption.19}}
\newlabel{fig:diff}{{5.3}{22}{The images illustrates the photometric reconstruction loss $ \mathcal {L}_{ps} $ for each pixel in a reconstructed image for the same frame but after different length of training. The left image shows the loss after 1 epoch of training and the right image shows the loss after 30 epochs. The reconstruction loss should decrease during training as the network learns to predict better depth maps, which is what we see.\relax }{figure.caption.19}{}}
\citation{monodepth2}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces The top row of images illustrates the edge weighting terms $e^{-|\delta _y \textbf  {I}_t|}$ and $e^{-|\delta _x \textbf  {I}_t|}$ respectively. The weight is near 0 at the edges of tree trunks and near 1 on the pavement. This will preserve the details in the depth map around trees but keep the pavement smooth.\relax }}{23}{figure.caption.20}}
\newlabel{fig:edge}{{5.4}{23}{The top row of images illustrates the edge weighting terms $e^{-|\delta _y \textbf {I}_t|}$ and $e^{-|\delta _x \textbf {I}_t|}$ respectively. The weight is near 0 at the edges of tree trunks and near 1 on the pavement. This will preserve the details in the depth map around trees but keep the pavement smooth.\relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.4}Depth map normalization}{23}{subsection.5.1.4}}
\newlabel{sec:normalization}{{5.1.4}{23}{Depth map normalization}{subsection.5.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.5}Depth map up-scaling}{23}{subsection.5.1.5}}
\newlabel{sec:upscale}{{5.1.5}{23}{Depth map up-scaling}{subsection.5.1.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces The depth maps can optionally be up-scaled in the loss function.\relax }}{24}{figure.caption.21}}
\newlabel{fig:upscale}{{5.5}{24}{The depth maps can optionally be up-scaled in the loss function.\relax }{figure.caption.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.6}Handling occlusions}{24}{subsection.5.1.6}}
\newlabel{sec:occlusion}{{5.1.6}{24}{Handling occlusions}{subsection.5.1.6}{}}
\citation{sfmlearner}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces By picking the per pixel minimum reprojection error the issue created by occluded pixels can be alleviated. The top image is the previous frame, the middle image is the current target frame and the bottom image is the next frame in the sequence. If the minimum reprojection error can be found in the previous frame then it is colored blue, if its from the next frame it is green. Because the door to the right is occluded by the orange truck in the previous frame, the reprojection loss from the next frame is used instead where the door is visible. The wall to the left is outside the boundaries of the next image, so the reprojection error from the previous frame is used instead.\relax }}{25}{figure.caption.22}}
\newlabel{fig:min}{{5.6}{25}{By picking the per pixel minimum reprojection error the issue created by occluded pixels can be alleviated. The top image is the previous frame, the middle image is the current target frame and the bottom image is the next frame in the sequence. If the minimum reprojection error can be found in the previous frame then it is colored blue, if its from the next frame it is green. Because the door to the right is occluded by the orange truck in the previous frame, the reprojection loss from the next frame is used instead where the door is visible. The wall to the left is outside the boundaries of the next image, so the reprojection error from the previous frame is used instead.\relax }{figure.caption.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.7}Handling model limitations}{25}{subsection.5.1.7}}
\newlabel{sec:modellimit}{{5.1.7}{25}{Handling model limitations}{subsection.5.1.7}{}}
\@writefile{toc}{\contentsline {paragraph}{Explainability mask}{25}{section*.23}}
\citation{monodepth2}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces This is an image extracted from the experiments in this thesis. The top image is frame $\textbf  {I}_t$ the middle image is frame $\textbf  {I}_{t-1}$ and the last image is the explain-ability mask. It is visible that the network correctly predicts that the bicycle is not moving in relation to the camera, but it does not remove pixels from the white van as it should.\relax }}{26}{figure.caption.24}}
\newlabel{fig:exp}{{5.7}{26}{This is an image extracted from the experiments in this thesis. The top image is frame $\textbf {I}_t$ the middle image is frame $\textbf {I}_{t-1}$ and the last image is the explain-ability mask. It is visible that the network correctly predicts that the bicycle is not moving in relation to the camera, but it does not remove pixels from the white van as it should.\relax }{figure.caption.24}{}}
\@writefile{toc}{\contentsline {paragraph}{Stationary pixels mask}{26}{section*.25}}
\citation{unsuperpoint}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces The black pixels in the image are the ones removed because their photometric error is smaller before warping the image using the depth map compared to after warping. The red horizontal lines on the van and bicyclist illustrates that they do not move with respect to the camera, and the green slanted line illustrates that the bike leaning on the wall is moving with respect to the camera. The mask successfully removes pixels on the van and bicyclist, but also removes some pixels on the pavement that should not be removed.\relax }}{27}{figure.caption.26}}
\newlabel{fig:stat}{{5.8}{27}{The black pixels in the image are the ones removed because their photometric error is smaller before warping the image using the depth map compared to after warping. The red horizontal lines on the van and bicyclist illustrates that they do not move with respect to the camera, and the green slanted line illustrates that the bike leaning on the wall is moving with respect to the camera. The mask successfully removes pixels on the van and bicyclist, but also removes some pixels on the pavement that should not be removed.\relax }{figure.caption.26}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Unsupervised keypoint learning}{27}{section.5.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces The network has a common backbone and then splits into separate score, position and descriptor encoders. The output is reshaped and sorted by descending score.\relax }}{28}{figure.caption.27}}
\newlabel{fig:unsuperpoint}{{5.9}{28}{The network has a common backbone and then splits into separate score, position and descriptor encoders. The output is reshaped and sorted by descending score.\relax }{figure.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces This figure illustrates the flow from input image to loss function. The image from branch A is fed directly into the UnsuperPoint network, while in branch B the image is first transformed by a random homography $\textbf  {H}$. The keypoint positions from twin A are transformed by the same homography $\textbf  {H}$ and the output from the two branches are compared in order to formulate the loss function.\relax }}{29}{figure.caption.28}}
\newlabel{fig:unsuperpointloss}{{5.10}{29}{This figure illustrates the flow from input image to loss function. The image from branch A is fed directly into the UnsuperPoint network, while in branch B the image is first transformed by a random homography $\textbf {H}$. The keypoint positions from twin A are transformed by the same homography $\textbf {H}$ and the output from the two branches are compared in order to formulate the loss function.\relax }{figure.caption.28}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Loss function for keypoint learning}{29}{subsection.5.2.1}}
\newlabel{sec:keypointloss}{{5.2.1}{29}{Loss function for keypoint learning}{subsection.5.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces Histogram with 50 buckets of scores for all points in a pair of images. The data is taken from the fully trained network evaluated in the results chapter. Most points has either a really low or high score, and a few points has a score somewhere in between.\relax }}{30}{figure.caption.29}}
\newlabel{fig:score-hist}{{5.11}{30}{Histogram with 50 buckets of scores for all points in a pair of images. The data is taken from the fully trained network evaluated in the results chapter. Most points has either a really low or high score, and a few points has a score somewhere in between.\relax }{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.12}{\ignorespaces Histogram with 100 buckets of relative point positions from both branch A and B, in both the $x$ and $y$ direction. As is clearly visible in the diagram there is a bias of relative positions towards 0 and 1.\relax }}{31}{figure.caption.30}}
\newlabel{fig:hist-no-unixy}{{5.12}{31}{Histogram with 100 buckets of relative point positions from both branch A and B, in both the $x$ and $y$ direction. As is clearly visible in the diagram there is a bias of relative positions towards 0 and 1.\relax }{figure.caption.30}{}}
\citation{pearsons}
\@writefile{lof}{\contentsline {figure}{\numberline {5.13}{\ignorespaces Histogram with 100 buckets of relative point positions when including the loss term $\mathcal  {L}^{\textrm  {uniform}}$ that encourages a uniform distribution. The data is taken from the model evaluated in the results chapter of this thesis.\relax }}{32}{figure.caption.31}}
\newlabel{fig:hist-unixy}{{5.13}{32}{Histogram with 100 buckets of relative point positions when including the loss term $\mathcal {L}^{\textrm {uniform}}$ that encourages a uniform distribution. The data is taken from the model evaluated in the results chapter of this thesis.\relax }{figure.caption.31}{}}
\citation{ransac}
\citation{consensus}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Unsupervised consensus maximization}{33}{section.5.3}}
\citation{pointnet}
\@writefile{lof}{\contentsline {figure}{\numberline {5.14}{\ignorespaces The input point positions $\mathbf  {X}\in \mathbb  {R}^{m\times n}$ are fed trough the PointNet segmentation network, and are also used to construct the Vandermonde matrix $\mathbf  {M}$. The rows of $\mathbf  {M}$ are weighted using the weights $\mathbf  {w}$. The network learns to predict inliers by minimizing the last singular values in $\mathbf  {\Sigma }$ in the loss function.\relax }}{35}{figure.caption.32}}
\newlabel{fig:consensus}{{5.14}{35}{The input point positions $\mathbf {X}\in \mathbb {R}^{m\times n}$ are fed trough the PointNet segmentation network, and are also used to construct the Vandermonde matrix $\mathbf {M}$. The rows of $\mathbf {M}$ are weighted using the weights $\mathbf {w}$. The network learns to predict inliers by minimizing the last singular values in $\mathbf {\Sigma }$ in the loss function.\relax }{figure.caption.32}{}}
\newlabel{eqn:H-1}{{5.33}{35}{Unsupervised consensus maximization}{equation.5.3.33}{}}
\newlabel{eqn:H-2}{{5.34}{35}{Unsupervised consensus maximization}{equation.5.3.34}{}}
\newlabel{eqn:H-3}{{5.35}{35}{Unsupervised consensus maximization}{equation.5.3.35}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces The basis monomials, the structure of $\textbf  {B}'$ in the new basis and its corresponding elements in $\textbf  {H}$.\relax }}{36}{table.caption.33}}
\newlabel{table:H}{{5.1}{36}{The basis monomials, the structure of $\textbf {B}'$ in the new basis and its corresponding elements in $\textbf {H}$.\relax }{table.caption.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.15}{\ignorespaces Histogram with 10 buckets of inlier weights $w_i$ for all point correspondences in a pair of images. A point is classified as an inlier if the weight $w_i$ is above a certain threshold.\relax }}{39}{figure.caption.34}}
\newlabel{fig:w-hist}{{5.15}{39}{Histogram with 10 buckets of inlier weights $w_i$ for all point correspondences in a pair of images. A point is classified as an inlier if the weight $w_i$ is above a certain threshold.\relax }{figure.caption.34}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Improving training convergence}{39}{subsection.5.3.1}}
\@setckpt{implementation}{
\setcounter{page}{41}
\setcounter{equation}{56}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{rt@toplevel}{0}
\setcounter{part}{0}
\setcounter{chapter}{5}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{15}
\setcounter{table}{1}
\setcounter{parentequation}{0}
\setcounter{CPROT@family}{1}
\setcounter{CPROT@family@temp}{0}
\setcounter{CPROT@series}{2}
\setcounter{CPROT@series@temp}{0}
\setcounter{CPROT@shape}{14}
\setcounter{CPROT@shape@temp}{0}
\setcounter{CPROT@size}{60}
\setcounter{CPROT@size@temp}{0}
\setcounter{CPROT@temp@chars}{0}
\setcounter{float@type}{4}
\setcounter{NAT@ctr}{26}
\setcounter{ContinuedFloat}{0}
\setcounter{Item}{2}
\setcounter{bookmark@seq@number}{39}
\setcounter{lofdepth}{1}
\setcounter{lotdepth}{1}
\setcounter{KVtest}{1}
\setcounter{subfigure}{0}
\setcounter{subfigure@save}{0}
\setcounter{subtable}{0}
\setcounter{subtable@save}{0}
\setcounter{rtpapers@section}{0}
\setcounter{rtpapers@chapter}{0}
\setcounter{theorem}{0}
\setcounter{Example}{0}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{section@level}{2}
}
