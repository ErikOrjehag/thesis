\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{sfmlearner}
\citation{dispnet}
\citation{resnet}
\citation{monodepth2}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Implementation}{17}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.1}Architectures for depth and ego motion CNNs}{17}{subsection.5.0.1}}
\citation{spatialtransformernetworks}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.2}Differentiable depth image warping}{18}{subsection.5.0.2}}
\newlabel{sec:diffwarp}{{5.0.2}{18}{Differentiable depth image warping}{subsection.5.0.2}{}}
\citation{ssim}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces The \textsc  {cnn}\xspace  predicts the depth map $D$ of the target image $I_t$, and also the relative movement, $T_{t\rightarrow t-1}, T_{t\rightarrow t+1}$ between the target image and the source images. Each pixel $p_t$ in the target image is projected onto a position in the source images which are sampled using bilinear interpolation. This should recreate the appearance of the target image but with pixels sampled from the source image. An appearance similarity metric between the original target image and the recreated target images can be used as the loss function for the \textsc  {cnn}\xspace  to learn to accurately predict correct depth and movement.\relax }}{19}{figure.caption.11}}
\newlabel{fig:warp}{{5.1}{19}{The \abbrCNN predicts the depth map $D$ of the target image $I_t$, and also the relative movement, $T_{t\rightarrow t-1}, T_{t\rightarrow t+1}$ between the target image and the source images. Each pixel $p_t$ in the target image is projected onto a position in the source images which are sampled using bilinear interpolation. This should recreate the appearance of the target image but with pixels sampled from the source image. An appearance similarity metric between the original target image and the recreated target images can be used as the loss function for the \abbrCNN to learn to accurately predict correct depth and movement.\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.3}Loss functions}{19}{subsection.5.0.3}}
\newlabel{sec:loss}{{5.0.3}{19}{Loss functions}{subsection.5.0.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces The images illustrates the photometric reconstruction loss $ \mathcal  {L}_{ps} $ with $\alpha =0.85$ for each pixel in the reconstructed image. The left image shows the loss after 1 epoch of training and the right image shows the loss after 30 epochs. The reconstruction loss should decrease during training as the network learns to predict better depth maps, which is what we see.\relax }}{20}{figure.caption.12}}
\newlabel{fig:diff}{{5.2}{20}{The images illustrates the photometric reconstruction loss $ \mathcal {L}_{ps} $ with $\alpha =0.85$ for each pixel in the reconstructed image. The left image shows the loss after 1 epoch of training and the right image shows the loss after 30 epochs. The reconstruction loss should decrease during training as the network learns to predict better depth maps, which is what we see.\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces The top row of images illustrates the edge weighting terms $e^{-|\delta _y I_t|}$ and $e^{-|\delta _x I_t|}$ respectively. The weight is near 0 at the edges of tree trunks and near 1 on the pavement. This will preserve the details in the depth map around trees but keep the pavement smooth.\relax }}{21}{figure.caption.13}}
\newlabel{fig:edge}{{5.3}{21}{The top row of images illustrates the edge weighting terms $e^{-|\delta _y I_t|}$ and $e^{-|\delta _x I_t|}$ respectively. The weight is near 0 at the edges of tree trunks and near 1 on the pavement. This will preserve the details in the depth map around trees but keep the pavement smooth.\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.4}Handling occlusions}{21}{subsection.5.0.4}}
\newlabel{sec:occlusion}{{5.0.4}{21}{Handling occlusions}{subsection.5.0.4}{}}
\citation{sfmlearner}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces By picking the per pixel minimum reprojection error the issue created by occluded pixels can be alleviated. The top image is the previous frame, the middle image is the current target frame and the bottom image is the next frame in the sequence. If the minimum reprojection error can be found in the previous frame then it is colored blue, if its from the next frame it is green. Because the door to the right is occluded by the orange truck in the previous frame, the reprojection loss from the next frame is used instead where the door is visible. The wall to the left is outside the boundaries of the next image, so the reprojection error from the previous frame is used instead.\relax }}{22}{figure.caption.14}}
\newlabel{fig:min}{{5.4}{22}{By picking the per pixel minimum reprojection error the issue created by occluded pixels can be alleviated. The top image is the previous frame, the middle image is the current target frame and the bottom image is the next frame in the sequence. If the minimum reprojection error can be found in the previous frame then it is colored blue, if its from the next frame it is green. Because the door to the right is occluded by the orange truck in the previous frame, the reprojection loss from the next frame is used instead where the door is visible. The wall to the left is outside the boundaries of the next image, so the reprojection error from the previous frame is used instead.\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.5}Handling model limitations}{22}{subsection.5.0.5}}
\newlabel{sec:modellimit}{{5.0.5}{22}{Handling model limitations}{subsection.5.0.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Explainability mask}{22}{section*.15}}
\citation{monodepth2}
\@writefile{toc}{\contentsline {paragraph}{Stationary pixels mask}{23}{section*.16}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces The model assumes a stationary world without moving objects in it. By excluding stationary pixels from the loss issues arising from moving objects can be alleviated. The black pixels in the image are the ones removed because their photometric error is smaller before warping the image using the depth map compared to after warping. The red horizontal lines on the van and bicyclist illustrates that they do not move with respect to the camera, and the green slanted line illustrates that the back bike tire is moving with respect to the camera. The mask successfully removes pixels on the van and bicyclist, but also removes some pixels on the pavement that should not be removed.\relax }}{23}{figure.caption.17}}
\newlabel{fig:stat}{{5.5}{23}{The model assumes a stationary world without moving objects in it. By excluding stationary pixels from the loss issues arising from moving objects can be alleviated. The black pixels in the image are the ones removed because their photometric error is smaller before warping the image using the depth map compared to after warping. The red horizontal lines on the van and bicyclist illustrates that they do not move with respect to the camera, and the green slanted line illustrates that the back bike tire is moving with respect to the camera. The mask successfully removes pixels on the van and bicyclist, but also removes some pixels on the pavement that should not be removed.\relax }{figure.caption.17}{}}
\citation{unsuperpoint}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Unsupervised keypoint learning}{24}{section.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces The network has a common backbone and then splits into separate score, position and descriptor encoders. The output is reshaped and sorted by descending score.\relax }}{24}{figure.caption.18}}
\newlabel{fig:unsuperpoint}{{5.6}{24}{The network has a common backbone and then splits into separate score, position and descriptor encoders. The output is reshaped and sorted by descending score.\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces This figure illustrates the flow from input image to loss function. The image from branch A is fed directly into the UnsuperPoint network, while in branch B the image is first transformed by a random homography $H$. The keypoint positions from twin A are transformed by the same homography $H$ and the output from the two branches are compared in order to formulate the loss function.\relax }}{25}{figure.caption.19}}
\newlabel{fig:unsuperpointloss}{{5.7}{25}{This figure illustrates the flow from input image to loss function. The image from branch A is fed directly into the UnsuperPoint network, while in branch B the image is first transformed by a random homography $H$. The keypoint positions from twin A are transformed by the same homography $H$ and the output from the two branches are compared in order to formulate the loss function.\relax }{figure.caption.19}{}}
\citation{ransac}
\citation{consensus}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Loss function for keypoint learning}{26}{subsection.5.1.1}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Unsupervised consensus maximization}{26}{section.5.2}}
\citation{pointnet}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces \relax }}{28}{figure.caption.20}}
\newlabel{fig:consensus}{{5.8}{28}{\relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Improving training convergence}{31}{subsection.5.2.1}}
\@setckpt{implementation}{
\setcounter{page}{32}
\setcounter{equation}{0}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{rt@toplevel}{0}
\setcounter{part}{0}
\setcounter{chapter}{5}
\setcounter{section}{2}
\setcounter{subsection}{1}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{8}
\setcounter{table}{0}
\setcounter{parentequation}{0}
\setcounter{CPROT@family}{1}
\setcounter{CPROT@family@temp}{0}
\setcounter{CPROT@series}{2}
\setcounter{CPROT@series@temp}{0}
\setcounter{CPROT@shape}{14}
\setcounter{CPROT@shape@temp}{0}
\setcounter{CPROT@size}{60}
\setcounter{CPROT@size@temp}{0}
\setcounter{CPROT@temp@chars}{0}
\setcounter{float@type}{4}
\setcounter{NAT@ctr}{16}
\setcounter{ContinuedFloat}{0}
\setcounter{Item}{2}
\setcounter{bookmark@seq@number}{35}
\setcounter{lofdepth}{1}
\setcounter{lotdepth}{1}
\setcounter{KVtest}{1}
\setcounter{subfigure}{0}
\setcounter{subfigure@save}{0}
\setcounter{subtable}{0}
\setcounter{subtable@save}{0}
\setcounter{rtpapers@section}{0}
\setcounter{rtpapers@chapter}{0}
\setcounter{theorem}{0}
\setcounter{Example}{0}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{section@level}{2}
}
