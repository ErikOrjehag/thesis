\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Results}{39}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{cha:results}{{6}{39}{Results}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Depth and ego motion}{39}{section.6.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces 3D render of colorized depth map\relax }}{39}{figure.caption.30}}
\newlabel{fig:3drender}{{6.1}{39}{3D render of colorized depth map\relax }{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces 3D visualization of the camera movement in three different image sequences. The green lines are the ground truth and the red lines are the predicted camera trajectories.\relax }}{40}{figure.caption.31}}
\newlabel{fig:movement}{{6.2}{40}{3D visualization of the camera movement in three different image sequences. The green lines are the ground truth and the red lines are the predicted camera trajectories.\relax }{figure.caption.31}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces All configurations of the depth and ego motion network that were evaluated. The C column identifies the specific configuration. The Net column shows which architecture was used, either SL for SfmLearner or M2 for Monodepth2. The DS column shows which dataset was used during training, either K for Kitti or L for Lyft. The Edge column indicates if the edge aware smoothing loss term $\mathcal  {L}_{edge}$ was used. The Norm column indicates if depth map normalization was used. The Expl column indicates if an explainability mask was used. The Stat column indicates if a mask to remove stationary pixels from the loss was used. The SSIM column indicates if $\mathcal  {L}_{ps}$ was used, otherwise just $\mathcal  {L}_{p}$. The Comb column shows which methods was used to combine the loss terms from the two source images, either the average or the minimum loss across frames. The US column indicates that up scaling of the depth maps in the pyramid was used, otherwise the target frame was down scaled to match the size of the smaller depth maps in the pyramid.\relax }}{40}{table.caption.32}}
\newlabel{table:configurations}{{6.1}{40}{All configurations of the depth and ego motion network that were evaluated. The C column identifies the specific configuration. The Net column shows which architecture was used, either SL for SfmLearner or M2 for Monodepth2. The DS column shows which dataset was used during training, either K for Kitti or L for Lyft. The Edge column indicates if the edge aware smoothing loss term $\mathcal {L}_{edge}$ was used. The Norm column indicates if depth map normalization was used. The Expl column indicates if an explainability mask was used. The Stat column indicates if a mask to remove stationary pixels from the loss was used. The SSIM column indicates if $\mathcal {L}_{ps}$ was used, otherwise just $\mathcal {L}_{p}$. The Comb column shows which methods was used to combine the loss terms from the two source images, either the average or the minimum loss across frames. The US column indicates that up scaling of the depth maps in the pyramid was used, otherwise the target frame was down scaled to match the size of the smaller depth maps in the pyramid.\relax }{table.caption.32}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces All the experiments measuring the performance of the different configurations in Table \ref  {table:configurations}. The E column identifies a specific experiment. The C column shows which configuration was used. The DS column shows which Dataset was used during testing, K for Kitti and L for Lyft. The dataset used during testing differs from the one used during training in some experiments. The AbsRel, SqRel, RMSE and RMSLE columns are the depth error metrics described in section \ref  {sec:depthmetrics}, smaller is better. The $1.25$, $1.25^2$ and $1.25^3$ columns are the depth accuracy metrics described in section \ref  {sec:depthmetrics}, larger is better. The Ego column is the camera ego motion error metric described in section \ref  {sec:egometric}.\relax }}{41}{table.caption.33}}
\newlabel{table:experiments}{{6.2}{41}{All the experiments measuring the performance of the different configurations in Table \ref {table:configurations}. The E column identifies a specific experiment. The C column shows which configuration was used. The DS column shows which Dataset was used during testing, K for Kitti and L for Lyft. The dataset used during testing differs from the one used during training in some experiments. The AbsRel, SqRel, RMSE and RMSLE columns are the depth error metrics described in section \ref {sec:depthmetrics}, smaller is better. The $1.25$, $1.25^2$ and $1.25^3$ columns are the depth accuracy metrics described in section \ref {sec:depthmetrics}, larger is better. The Ego column is the camera ego motion error metric described in section \ref {sec:egometric}.\relax }{table.caption.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Examples from the Kitti dataset\relax }}{42}{figure.caption.34}}
\newlabel{fig:depthmapskitty}{{6.3}{42}{Examples from the Kitti dataset\relax }{figure.caption.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Examples from the Lyft dataset\relax }}{42}{figure.caption.35}}
\newlabel{fig:depthmaplyft}{{6.4}{42}{Examples from the Lyft dataset\relax }{figure.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces 3D visualization from 3 different angles of the same frame in the kitti dataset.\relax }}{43}{figure.caption.36}}
\newlabel{fig:3dseq}{{6.5}{43}{3D visualization from 3 different angles of the same frame in the kitti dataset.\relax }{figure.caption.36}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Keypoint detection}{43}{section.6.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces Results from the keypoint prediction network. The top row are the original input images fed to branch A, and bellow are the transformed images fed to branch B. Circles that are the same color have matching descriptors.\relax }}{43}{figure.caption.37}}
\newlabel{fig:point1}{{6.6}{43}{Results from the keypoint prediction network. The top row are the original input images fed to branch A, and bellow are the transformed images fed to branch B. Circles that are the same color have matching descriptors.\relax }{figure.caption.37}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces Experiments measuring the performance of keypoint detection methods.\relax }}{44}{table.caption.38}}
\newlabel{table:pointsbenchmark}{{6.3}{44}{Experiments measuring the performance of keypoint detection methods.\relax }{table.caption.38}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Consensus maximization}{44}{section.6.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces Outlier detection and homography estimation on images from kitti dataset. First row is from OpenCV findHomography(), second row is from the network, third row is the image from branch A transformed by the homographgy found by OpenCV, fourth row is the image from branch A transformed by the homography found by the network.\relax }}{44}{figure.caption.39}}
\newlabel{fig:kittihomo}{{6.7}{44}{Outlier detection and homography estimation on images from kitti dataset. First row is from OpenCV findHomography(), second row is from the network, third row is the image from branch A transformed by the homographgy found by OpenCV, fourth row is the image from branch A transformed by the homography found by the network.\relax }{figure.caption.39}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.4}{\ignorespaces Metrics for homography estimation.\relax }}{44}{table.caption.40}}
\newlabel{table:pointsbenchmark}{{6.4}{44}{Metrics for homography estimation.\relax }{table.caption.40}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.5}{\ignorespaces Confusion matrix for inlier/outlier prediction using ConsensusNet.\relax }}{45}{table.caption.41}}
\newlabel{table:consensusnetconfusion}{{6.5}{45}{Confusion matrix for inlier/outlier prediction using ConsensusNet.\relax }{table.caption.41}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.6}{\ignorespaces Confusion matrix for inlier/outlier prediction using RANSAC.\relax }}{45}{table.caption.42}}
\newlabel{table:ransacconfusion}{{6.6}{45}{Confusion matrix for inlier/outlier prediction using RANSAC.\relax }{table.caption.42}{}}
\@setckpt{results}{
\setcounter{page}{46}
\setcounter{equation}{0}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{rt@toplevel}{0}
\setcounter{part}{0}
\setcounter{chapter}{6}
\setcounter{section}{3}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{7}
\setcounter{table}{6}
\setcounter{parentequation}{0}
\setcounter{CPROT@family}{1}
\setcounter{CPROT@family@temp}{0}
\setcounter{CPROT@series}{2}
\setcounter{CPROT@series@temp}{0}
\setcounter{CPROT@shape}{14}
\setcounter{CPROT@shape@temp}{0}
\setcounter{CPROT@size}{60}
\setcounter{CPROT@size@temp}{0}
\setcounter{CPROT@temp@chars}{0}
\setcounter{float@type}{4}
\setcounter{NAT@ctr}{25}
\setcounter{ContinuedFloat}{0}
\setcounter{Item}{2}
\setcounter{bookmark@seq@number}{42}
\setcounter{lofdepth}{1}
\setcounter{lotdepth}{1}
\setcounter{KVtest}{1}
\setcounter{subfigure}{0}
\setcounter{subfigure@save}{0}
\setcounter{subtable}{0}
\setcounter{subtable@save}{0}
\setcounter{rtpapers@section}{0}
\setcounter{rtpapers@chapter}{0}
\setcounter{theorem}{0}
\setcounter{Example}{0}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{section@level}{1}
}
