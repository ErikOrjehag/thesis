\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Results}{41}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{cha:results}{{6}{41}{Results}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Depth and ego motion}{41}{section.6.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Three depth maps from experiment E13, which is the Monodepth2 architecture with all options enabled, trained and evaluated on the Kitty dataset.\relax }}{42}{figure.caption.34}}
\newlabel{fig:depthmapskitty}{{6.1}{42}{Three depth maps from experiment E13, which is the Monodepth2 architecture with all options enabled, trained and evaluated on the Kitty dataset.\relax }{figure.caption.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces One depth map from experiment E13 rendered as a colorized point cloud.\relax }}{42}{figure.caption.35}}
\newlabel{fig:3drender}{{6.2}{42}{One depth map from experiment E13 rendered as a colorized point cloud.\relax }{figure.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces A single depth map from experiment E13 rendered from 3 different angles as a colorized point cloud.\relax }}{42}{figure.caption.36}}
\newlabel{fig:3dseq}{{6.3}{42}{A single depth map from experiment E13 rendered from 3 different angles as a colorized point cloud.\relax }{figure.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Camera movement in three different image sequences from experiment E13. The green lines are the ground truth and the red lines are the predicted camera trajectories.\relax }}{43}{figure.caption.37}}
\newlabel{fig:movement}{{6.4}{43}{Camera movement in three different image sequences from experiment E13. The green lines are the ground truth and the red lines are the predicted camera trajectories.\relax }{figure.caption.37}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces All configurations of the depth and ego motion network that were evaluated. The C column identifies the specific configuration. The Net column shows which architecture was used, either SL for SfmLearner or M2 for Monodepth2. The DS column shows which dataset was used during training, either K for Kitti or L for Lyft. The Edge column indicates if the edge aware smoothing loss term $\mathcal  {L}_{edge}$ was used. The Norm column indicates if depth map normalization was used. The Expl column indicates if an explainability mask was used. The Stat column indicates if a mask to remove stationary pixels from the loss was used. The SSIM column indicates if $\mathcal  {L}_{ps}$ was used, otherwise just $\mathcal  {L}_{p}$. The Comb column shows which methods was used to combine the loss terms from the two source images, either the average or the minimum loss across frames. The US column indicates that up scaling of the depth maps in the pyramid was used, otherwise the target frame was down scaled to match the size of the smaller depth maps in the pyramid.\relax }}{43}{table.caption.38}}
\newlabel{table:configurations}{{6.1}{43}{All configurations of the depth and ego motion network that were evaluated. The C column identifies the specific configuration. The Net column shows which architecture was used, either SL for SfmLearner or M2 for Monodepth2. The DS column shows which dataset was used during training, either K for Kitti or L for Lyft. The Edge column indicates if the edge aware smoothing loss term $\mathcal {L}_{edge}$ was used. The Norm column indicates if depth map normalization was used. The Expl column indicates if an explainability mask was used. The Stat column indicates if a mask to remove stationary pixels from the loss was used. The SSIM column indicates if $\mathcal {L}_{ps}$ was used, otherwise just $\mathcal {L}_{p}$. The Comb column shows which methods was used to combine the loss terms from the two source images, either the average or the minimum loss across frames. The US column indicates that up scaling of the depth maps in the pyramid was used, otherwise the target frame was down scaled to match the size of the smaller depth maps in the pyramid.\relax }{table.caption.38}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces All the experiments measuring the performance of the different configurations in Table \ref  {table:configurations}. The E column identifies a specific experiment. The C column shows which configuration was used. The DS column shows which dataset was used during testing, K for Kitti and L for Lyft. The dataset used during testing differs from the one used during training in some experiments. The AbsRel, SqRel, RMSE and RMSLE columns are the depth error metrics described in section \ref  {sec:depthmetrics}, smaller is better. The $1.25$, $1.25^2$ and $1.25^3$ columns are the depth accuracy metrics described in section \ref  {sec:depthmetrics}, larger is better. The Ego column is the camera ego motion error metric described in section \ref  {sec:egometric}.\relax }}{44}{table.caption.39}}
\newlabel{table:experiments}{{6.2}{44}{All the experiments measuring the performance of the different configurations in Table \ref {table:configurations}. The E column identifies a specific experiment. The C column shows which configuration was used. The DS column shows which dataset was used during testing, K for Kitti and L for Lyft. The dataset used during testing differs from the one used during training in some experiments. The AbsRel, SqRel, RMSE and RMSLE columns are the depth error metrics described in section \ref {sec:depthmetrics}, smaller is better. The $1.25$, $1.25^2$ and $1.25^3$ columns are the depth accuracy metrics described in section \ref {sec:depthmetrics}, larger is better. The Ego column is the camera ego motion error metric described in section \ref {sec:egometric}.\relax }{table.caption.39}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces The top row is from experiment E1, the second row is from E2 with the explainability maps shown to the left, and the last row is from E3 with the stationary pixel map shown to the left. In the top depth map the depth prediction for the biker is incorrect. Because the biker is stationary with respect to the camera the disparity of the pixels across the subsequent frames becomes 0 and the depth goes to infinity. Training using an explain-ability mask seems to improve the depth prediction for the biker, and using a stationary pixels mask improves the depth even more.\relax }}{45}{figure.caption.40}}
\newlabel{fig:e1e2e3}{{6.5}{45}{The top row is from experiment E1, the second row is from E2 with the explainability maps shown to the left, and the last row is from E3 with the stationary pixel map shown to the left. In the top depth map the depth prediction for the biker is incorrect. Because the biker is stationary with respect to the camera the disparity of the pixels across the subsequent frames becomes 0 and the depth goes to infinity. Training using an explain-ability mask seems to improve the depth prediction for the biker, and using a stationary pixels mask improves the depth even more.\relax }{figure.caption.40}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces A depth map taken from experiment E4 where the edge aware smoothness loss is used.\relax }}{45}{figure.caption.41}}
\newlabel{fig:E4}{{6.6}{45}{A depth map taken from experiment E4 where the edge aware smoothness loss is used.\relax }{figure.caption.41}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces Depth map from experiment E14, where the model was trained on Kitti and evaluated on Lyft. As can be seen in the image the performance is still quite good.\relax }}{46}{figure.caption.42}}
\newlabel{fig:E14}{{6.7}{46}{Depth map from experiment E14, where the model was trained on Kitti and evaluated on Lyft. As can be seen in the image the performance is still quite good.\relax }{figure.caption.42}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces Depth map from experiment E15 where the model was trained on Lyft and also evaluated on Lyft.\relax }}{46}{figure.caption.43}}
\newlabel{fig:E16}{{6.8}{46}{Depth map from experiment E15 where the model was trained on Lyft and also evaluated on Lyft.\relax }{figure.caption.43}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Keypoint detection}{46}{section.6.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.9}{\ignorespaces Results from the keypoint prediction network. The top row are the original input images fed to branch A, and bellow are the transformed images fed to branch B. Circles that are the same color have matching descriptors.\relax }}{46}{figure.caption.44}}
\newlabel{fig:point1}{{6.9}{46}{Results from the keypoint prediction network. The top row are the original input images fed to branch A, and bellow are the transformed images fed to branch B. Circles that are the same color have matching descriptors.\relax }{figure.caption.44}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces Comparison between the keypoint predicting network and ORB detector in OpenCV. The table shows the repeatability score, localization error, matching score and matching ratio.\relax }}{46}{table.caption.45}}
\newlabel{table:pointsbenchmark}{{6.3}{46}{Comparison between the keypoint predicting network and ORB detector in OpenCV. The table shows the repeatability score, localization error, matching score and matching ratio.\relax }{table.caption.45}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Consensus maximization}{47}{section.6.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.10}{\ignorespaces Outl homographic adaptation dataset with images from Kitti. First row is from OpenCV findHomography(), second row is from the consensus network, third row is the image from branch A transformed by the homographgy found by OpenCV, fourth row is the image from branch A transformed by the homography found by the consensus network.\relax }}{47}{figure.caption.46}}
\newlabel{fig:kittihomo}{{6.10}{47}{Outl homographic adaptation dataset with images from Kitti. First row is from OpenCV findHomography(), second row is from the consensus network, third row is the image from branch A transformed by the homographgy found by OpenCV, fourth row is the image from branch A transformed by the homography found by the consensus network.\relax }{figure.caption.46}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.4}{\ignorespaces Comparison between the consensus maximization network and findHomography() function of OpenCV which uses the RANSAC algorithm.\relax }}{47}{table.caption.47}}
\newlabel{table:consensusbenchmark}{{6.4}{47}{Comparison between the consensus maximization network and findHomography() function of OpenCV which uses the RANSAC algorithm.\relax }{table.caption.47}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.5}{\ignorespaces Confusion matrix for inlier/outlier prediction using ConsensusNet.\relax }}{48}{table.caption.48}}
\newlabel{table:consensusnetconfusion}{{6.5}{48}{Confusion matrix for inlier/outlier prediction using ConsensusNet.\relax }{table.caption.48}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.6}{\ignorespaces Confusion matrix for inlier/outlier prediction using RANSAC.\relax }}{48}{table.caption.49}}
\newlabel{table:ransacconfusion}{{6.6}{48}{Confusion matrix for inlier/outlier prediction using RANSAC.\relax }{table.caption.49}{}}
\@setckpt{results}{
\setcounter{page}{49}
\setcounter{equation}{0}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{rt@toplevel}{0}
\setcounter{part}{0}
\setcounter{chapter}{6}
\setcounter{section}{3}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{10}
\setcounter{table}{6}
\setcounter{parentequation}{0}
\setcounter{CPROT@family}{1}
\setcounter{CPROT@family@temp}{0}
\setcounter{CPROT@series}{2}
\setcounter{CPROT@series@temp}{0}
\setcounter{CPROT@shape}{14}
\setcounter{CPROT@shape@temp}{0}
\setcounter{CPROT@size}{60}
\setcounter{CPROT@size@temp}{0}
\setcounter{CPROT@temp@chars}{0}
\setcounter{float@type}{4}
\setcounter{NAT@ctr}{26}
\setcounter{ContinuedFloat}{0}
\setcounter{Item}{2}
\setcounter{bookmark@seq@number}{42}
\setcounter{lofdepth}{1}
\setcounter{lotdepth}{1}
\setcounter{KVtest}{1}
\setcounter{subfigure}{0}
\setcounter{subfigure@save}{0}
\setcounter{subtable}{0}
\setcounter{subtable@save}{0}
\setcounter{rtpapers@section}{0}
\setcounter{rtpapers@chapter}{0}
\setcounter{theorem}{0}
\setcounter{Example}{0}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{section@level}{1}
}
