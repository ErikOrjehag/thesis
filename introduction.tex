\chapter{Introduction}\label{cha:introduction}

The aim of this thesis is to investigate the performance and implementation of some recently published techniques that use unsupervised training of neural networks in the structure from motion pipeline. 

Imagine closing one eye and looking out into the world. It is trivial as a grownup human to detect motion and estimate how the head moves in relation to what is seen. Calculating camera movement from an image sequence is a well studied problem and is usually done by finding corresponding features in the images and calculating (using projective geometry) which camera movement can give rise to such correspondences and their relative movement between frames in the sequence.

Recent research has shown that it is possible to predict depth and relative motion from a sequence of images taken with a monocular \abbrRGB camera, up to an unknown scale factor. The training data is a sequence of unlabeled images with a small relative motion, for example looking out from the front window of a moving car. Given a target view and a few nearby views it is possible to train depth and pose predicting \abbrCNN{}s jointly using a combined loss function. The depth and pose predictions are used to warp nearby views to the target view and the loss is based on the visual similarity achieved after warping.

In addition to predicting depth and motion in an image sequence, it is also useful to extract feature points in the image that can be tracked over time in order to build a map of the world. In the process of extracting feature points, the system should be able to filter out points that are matched incorrectly with each other, so called outliers.

If depth prediction, camera motion, feature point detection and geometric consensus maximization are implemented in an unsupervised training framework it opens up the possibility to train all subsystems jointly where each part benefits from the rest. Collecting training data would be cheap, since no manual annotation of the data is needed. But this is left as an area of future work and not covered in this thesis.

\section{Motivation}

Here are three main points motivating future research into the use of unsupervised learning methods for visual structure from motion.


\begin{itemize}
	\item Localization of autonomous vehicles is commonly  achieved using lidar sensors due to their high accuracy and robustness, but using cameras instead comes with many other benefits\cite{lidarvscamera}. For example, lower hardware cost and power consumption. Cameras are passive sensors which means that they do not interfere with each other. Lidars rely on spinning parts which can break if subjected to shaking or impact. Focusing on monoscopic vision instead of stereo vision again comes down to cost benefits.
	
	\item Obtaining labeled data for supervised training can be a tedious task, in that respect unsupervised methods are much more desirable. Labeling data is time consuming, expensive and prone to human errors. Collecting and storing data without labeling it is however comparably easy and inexpensive.
	
	\item This thesis focuses on depth, ego motion, keypoint prediction and consensus maximization. The motivation for this choice of unsupervised methods is the idea that in future work it might be possible to combine these methods to jointly learn all the tasks at the same time where each part of the system benefits from the others during training. The hope would be that the performance of the system would increase simply by collecting more unlabeled data.
\end{itemize}

\section{Research Questions}

\begin{enumerate}
	
	\item How well do the unsupervised methods from previous research work on new datasets not tested in the original papers?
	
	\item What are the performance gains of combining different methods from recent research in monocular depth and ego motion prediction?
	
\end{enumerate}

\section{Delimitations}

The visual localization problem can be solved using, for example, a stereoscopic camera or a time of flight camera. But this thesis will only explore the use of a monoscopic, non depth sensing, \abbrRGB camera.

A full \abbrSFM pipeline can be conceptually divided into two parts, perception and map estimation. The perception block would process the sensor data and extract for example depth information, a pose update, and keypoint features that can be tracked over time. This is bundled as a keyframe that is passed on to the map estimation block that is responsible for building a map that is consistent over time with minimal drift\cite{orbslam}.

This thesis will primarily focus on how unsupervised training can be applied to the perception block, including depth estimation and feature tracking, and will not be investigating anything to do with map estimation.

A few different unsupervised learning methods to predict depth and camera motion from a sequence of unlabeled images will be investigated. Additionally two specific unsupervised learning methods will be evaluated. Firstly how to predict feature points and their descriptors, and secondly how to perform geometric consensus maximization on the corresponding points.