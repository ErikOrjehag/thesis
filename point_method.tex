\section{Unsupervised keypoint learning}

In the previous chapter we saw how neural networks can predict depth and ego motion. This can be seen as implicitly matching all the pixels in one frame with corresponding pixels in a nearby frame. But in order to build a map of the environment it is useful to extract sparse and repeatable features, or keypoints, from images. Every keypoint should have a unique descriptor which can be used to identify it in the map. This would make it possible to expand the map and also localize the camera. How to build a map and localize in it is out of scope for this thesis. This section will describe the usnupervised learning method implemented in this thesis, to extract keypoints from images.

The method evaluated in this thesis is based on the UnsuperPoint paper\cite{unsuperpoint}. The network architecture is illustrated in Figure \ref{fig:unsuperpoint}. The input image is fed into a shared backbone. The features from the backbone are then split into three different encoders that estimate the score, position and a descriptor for each keypoint. The network will estimate a keypoint in every $8\times 8$ patch of the image, but only the top $N$ keypoints sorted by score are used in the evaluation.

\begin{figure}[H]
	\centering
	\includegraphics[width=1.0\textwidth]{unsuperpoint}
	\caption{The network has a common backbone and then splits into separate score, position, and descriptor encoders. The output is reshaped and sorted by descending score.}
	\label{fig:unsuperpoint}
\end{figure}

The score encoder is terminated by a sigmoid function and outputs $\textbf{S}_{\mathrm{map}}$, containing scores between 0 and 1 for each keypoint. The purpose of the scores are to rank the quality of the keypoints in all $8\times 8$ px patches of the image. A subset of the keypoints with the highest scores are safe to use in the SFM system, while the keypoints with low scores are disregarded. Typically patches in the sky and other non-textured areas will have keypoints with low scores.

The position encoder is also terminated by a sigmoid function and outputs $\textbf{P}_{\textrm{relative}}$ which is the relative position of the keypoint in each $8 \times 8$ patch. In the Map2XY block in Figure \ref{fig:unsuperpoint} the relative positions are converted to absolute pixel positions to form $\textbf{P}_{\textrm{map}}$. The matrices have $H/8$ rows and $W/8$ columns, with two channels per cell storing the continuous x and y position for a predicted point in each patch. To convert from from relative to absolute point positions, the following equations are used, where $r$ is each row and $c$ is each column in the matrices:

\begin{equation}
\textbf{P}_{\mathrm{map,x}}(r,c) = 8 * (c + \textbf{P}_{\mathrm{relative,x}}(r,c))
\end{equation}
\begin{equation}
\textbf{P}_{\mathrm{map,y}}(r,c) = 8 * (r + \textbf{P}_{\mathrm{relative,y}}(r,c))
\end{equation}

The descriptor encoder predicts a descriptor vector of length $256$ for each keypoint. The purpose of the descriptor is to find corresponding keypoints in different images. The encoder produces 1 descriptor vector for each $8\times 8$ patch in the image. This vector can be used directly as the keypoint descriptor and it works pretty well. Even better results can be achieved using the absolute keypoint positions to sample the values in the descriptor map with the same interpolation method used to do the warping in Figure \ref{fig:warp}.

$\textbf{S}_{\textrm{map}}$, $\textbf{P}_{\mathrm{map}}$ and $\textbf{F}_{\mathrm{map}}$ are reshaped into a vector $\textbf{s}$ of $M$ elements, an $M\times 2$ matrix $\textbf{P}$ and an $M\times 255$ matrix $\textbf{F}$, where $M=\frac{W}{8}*\frac{H}{8}=832$.

The network is trained in a Siamese network setup, where two duplicate networks that share weights are fed different inputs and the loss is formulated by comparing the output scores, positions and descriptors. The flow of data is illustrated in Figure~\ref{fig:unsuperpointloss}.

\begin{figure}[H]
	\centering
	\includegraphics[width=1.0\textwidth]{unsuperpointloss}
	\caption{This figure illustrates the flow from input image to loss function. The image from branch A is fed directly into the UnsuperPoint network, while in branch B the image is first transformed by a random homography $\textbf{H}$. The keypoint positions from twin A are transformed by the same homography $\textbf{H}$ and the output from the two branches are compared in order to formulate the loss function. Note that the image transformation by $\textbf{H}$ in the figure, in practise becomes a resampling by $\textbf{H}^{-1}$.}
	\label{fig:unsuperpointloss}
\end{figure}

For each branch $b\in\{A,B\}$ the Siamese networks output the reshaped matrices $\textbf{s}^b$, $\textbf{P}^b$ and $\textbf{F}^b$ which contain the scores, positions and descriptors of the $M$ points in each image.

\subsection{Loss function for keypoint learning}\label{sec:keypointloss}

To formulate the loss function, the point correspondences between branch A and B need to be determined. The points in branch A are transformed such that $\textbf{p}_i^{A\rightarrow B}=\textbf{H}\textbf{p}_i^A$. Then an $M^A\times M^B$ distance matrix $\textbf{G}$ is calculated from the pairwise distances between all points in each branch.

\begin{equation}
\textbf{G}=[g_{ij}]_{M^A\times M^B}=\left[||\textbf{p}_i^{A\rightarrow B}-\textbf{p}_j^B||\right]_{M^A\times M^B}
\end{equation}

We define a point pair if point $i$ in branch A has a point $j$ as its closest neighbor in branch B, and if the distance $g_{ij}$ is less than 8px. With the point pairs defined, the output matrices $\textbf{s}^b$, $\textbf{P}^b$ and $\textbf{F}^b$ can be redefined as \textit{corresponding matrices} $\hat{\textbf{s}}^b$, $\hat{\textbf{P}}^b$ and $\hat{\textbf{F}}^b$ with $K\le M$ entries, such that entry $k$ in the new matrices map to corresponding points in the input images.

Define $d_k$ as the distance between each point pair.

\begin{equation}
d_k=||\hat{\textbf{p}}_k^{A\rightarrow B}-\hat{\textbf{p}}_k^B||
\end{equation}

The distance between point pairs should be minimized, this is achieved by the $\mathcal{L}^{\textrm{position}}$ loss term.

\begin{equation}
\mathcal{L}_k^{\textrm{position}} = d_k
\end{equation}

The scores of point pairs in branch A and B should be similar, this is achieved by the $\mathcal{L}^{\textrm{sim}}$ loss term.

\begin{equation}
\mathcal{L}_k^{\textrm{sim}} = \left(\hat{\textbf{s}}_k^{A}-\hat{\textbf{s}}_k^B\right)^2
\end{equation}

To teach the network to predict sensible scores for the points, the distance $d_k$ between point pairs is used.

\begin{equation}
\mathcal{L}_k^{\textrm{score}}=\frac{\hat{\textbf{s}}_k^A+\hat{\textbf{s}}_k^B}{2}\left(d_k-\bar{d}\right)
\end{equation}

If $d_k$ is less than the mean distance $\bar{d}$ the score should be large in order to minimize the loss. If the $d_k$ is greater than the mean distance $\bar{d}$ the score should be small in order to minimize the loss. Figure~\ref{fig:score-hist} shows the distribution of scores predicted by a fully trained network.

\begin{figure}[H]
	\begin{center}
		\input{fig/hist-score.pgf}
	\end{center}
	\caption{Histogram with 50 buckets of scores for all points in a pair of images. The data is taken from the fully trained network evaluated in the results chapter. Most points have either a really low or high score, and a few points have a score somewhere in-between.}
	\label{fig:score-hist}
\end{figure}

Training with only the above mentioned loss terms, the relative positions of the points in the $8\times 8$ patches will be distributed towards the boundaries. One explanation for this is that hardly repeatable points will be encouraged to position themself near points in their neighboring patches, thereby minimizing $d_k$. The problem is illustrated in Figure \ref{fig:hist-no-unixy}.

\begin{figure}[H]
	\begin{center}
		\input{fig/hist-no-unixy.pgf}
	\end{center}
	\caption{Histogram with 100 buckets of relative point positions from both branch A and B, in both the $x$ and $y$ direction. As is clearly visible in the diagram there is a bias of relative positions towards 0 and 1.}
	\label{fig:hist-no-unixy}
\end{figure}

To alleviate this issue, a loss is added that encourages uniform distribution of the relative point positions.


\begin{equation}
\mathcal{L}^{\textrm{uniform}}=\mathcal{L}^{\textrm{uniform\_x}}_A+\mathcal{L}^{\textrm{uniform\_y}}_A+\mathcal{L}^{\textrm{uniform\_x}}_B+\mathcal{L}^{\textrm{uniform\_y}}_B
\end{equation}
where
\begin{equation}
\mathcal{L}^{\textrm{uniform\_x}}_A=\sum_{i=1}^M\left(x_i^{\textrm{sorted}}-\frac{i-1}{M-1}\right)^2
\end{equation}
and so on for both branch A and B, in the $x$ and $y$ dimension. The effect of applying this loss can be seen in Figure \ref{fig:hist-unixy}.

\begin{figure}[H]
	\begin{center}
		\input{fig/hist-unixy.pgf}
	\end{center}
	\caption{Histogram with 100 buckets of relative point positions when including the loss term $\mathcal{L}^{\textrm{uniform}}$ that encourages a uniform distribution. The data is taken from the model evaluated in the results chapter of this thesis.}
	\label{fig:hist-unixy}
\end{figure}

The descriptor loss is calculated using a hinge loss with both positive margin $m_p$ and negative margin $m_n$. An $M^A\times M^B$ correspondence matrix $\textbf{C}$ is constructed, such that

\begin{equation}
c_{ij}=
\begin{cases}
1\ \text{if}\ g_{ij}\le 8 \\
0\ \text{otherwise.} \\
\end{cases}
\end{equation}

A maximum distance of 8px to classify two points as corresponding is suitable because of the patch size of $8\times 8$ pixels. Unlike point pairs, a single point can correspond to multiple points in the other branch. The descriptor loss term is defined using the correspondance matrix $\textbf{C}$ and descriptor matrix $\textbf{F}$ as follows:

\begin{equation}
\mathcal{L}^{\textrm{desc}}=\sum^{M^B}_{i=1}\sum^{M^B}_{j=1}\lambda_d*c_{ij}\max\left(0,m_p-{\textbf{f}_i^A}^T\textbf{f}_j^B\right)+(1-c_{ij})\max\left(0,{\textbf{f}_i^A}^T\textbf{f}_j^B-m_n\right)
\end{equation}

Where $\textbf{f}^b_i$ is row $i$ of $\textbf{F}^b$ for each branch $b\in\{A,B\}$. The weight $\lambda_d$ is used to balance the few corresponding points compared to the non-corresponding ones. The descriptors are decorrelated by minimizing the off-diagonal elements of a descriptor correlation matrix $\textbf{R}^b=[r^b_{ij}]_{F\times F}$.

\begin{equation}
\mathcal{L}^{\textrm{decorr}}=\sum^{\textbf{R}}_{i\neq j}r_{ij}^A+\sum^{\textbf{R}}_{i\neq j}r_{ij}^B
\end{equation}
Each element $r^b_{ij}$ for $b\in\{A,B\}$ is a Pearson's correlation coefficient\cite{pearsons} defined as
\begin{equation}
r^b_{ij}=
\frac{ \tilde{\textbf{f}}_i^b \cdot \tilde{\textbf{f}}_j^b }{
||\tilde{\textbf{f}}_i^b||\ ||\tilde{\textbf{f}}_j^b||
}
,\ \text{where}\ 
\tilde{\textbf{f}}_i^b=\textbf{f}_i^b-\bar{\textbf{f}}^b_i
\ \text{and}\ 
\tilde{\textbf{f}}_j^b=\textbf{f}_j^b-\bar{\textbf{f}}^b_j
\end{equation}

Where $\bar{\textbf{f}}^b_i$ is the mean of vector $\textbf{f}^b_i$. Due to what might be just an oversight by the authors, this is not the same definition of $r_{ij}$ as presented in the original UnsuperPoint paper.