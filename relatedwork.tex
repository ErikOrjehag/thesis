\chapter{Related work}\label{cha:relatedwork}


In this chapter some of the important contributions of previous research papers are summarized.

\section{Unsupervised Monocular Depth Estimation with Left-Right Consistency \cite{leftright}}
\label{sec:relwork:leftright}
In a paper from 12 Apr 2017 the authors present MonoDepth, with an implementation available in Tensorflow on github. In this work the depth is predicted using an encoder-decoder type network, but the relative motion between frames is not estimated at all. The KITTI dataset provides stereo image pairs which are used during training, and the relative transformation between the left and right cameras is known. Using only the left image as input to the network both disparity maps for the left and right images are predicted. The two disparity maps are used to project the left image into the view of the right image and vice versa. This can be seen as a precursor to the papers discussed later which uses only a monocular image sequence to train a depth predicting network and pose predicting network jointly.

In the loss function, the left image is reconstructed from the right image and vice versa. This is possible using the disparity maps and known baseline and focal length of the cameras. The L1 norm of the per pixel photometric error as well as \abbrSSIM are computed and added to the loss. An additional loss term encourages the left disparity to be equal to the right disparity projected into the left camera viewpoint. Because the photometric error does not work well on low textured regions an edge aware smoothess term is added to propagate the depth values from nearby areas in the disparity map. The method produces metrically accurate results because the baseline and focal length of the cameras are known.

\section{Unsupervised Learning of Depth and Ego-Motion from Video \cite{sfmlearner}}
\label{sec:relwork:unego}
In a paper from CVPR 21 July 2017 the authors present SfMLearner with an official implementation in Tensorflow on github. Additional replications in the PyTorch and Chainer frameworks are also available.

Contrary to the paper in section \ref{sec:relwork:leftright} only a monocular sequence of images from the KITTI dataset is used during training. In the stereo case the relative pose between the left and right cameras was known, but with this monocular dataset the pose between subsequent frames is unknown. The authors train a pose predicting and depth predicting network simultaneously with a joint loss function to solve this problem.

During training 3 subsequent frames are considered at a time. The frame $I_{t-1}$ and frame $I_{t+1}$ are called the source frames and the frame $I_t$ is called the target frame. The target frame is input to the depth network which estimates a disparity map. The two source frames are fed through a pose estimating network one after each other together with the target frame to find the relative transformations $T_{t	\rightarrow t-1}$ and $T_{t	\rightarrow t+1}$.

The authors add an explainability mask to the photometric error term to account for errors in the model. The view synthesis formulation implicitly assumes that the scene does not contain moving objects, that there are no occlusions between the target and source frames, and that the surfaces are Lambertian so that the photo-consistency error of \abbrRGB values is meaningful. In order to predict the explainability mask an additional \abbrCNN is used. The network has no explicit supervisory signal but is encouraged to be non-zero with an regularization term using a cross-entropy loss with a constant label 1 at each pixel location. This makes the network minimize the view synthesis objective but is allowed some slack due to factors not considered by the model. In later work it was shown that the explainability mask does not help to improve results that much and is often ignored.

To tackle the problem with textured areas and non Lambertian surfaces, a smoothess term is used. An edge aware smoothess term was not used like in previous work\cite{leftright} but a penalty on the second order gradient of the depth map was used instead. This unfortunately makes edges very fuzzy in the results compared to other methods.

\section{SuperDepth: Self-Supervised, Super-Resolved Monocular Depth Estimation \cite{superdepth}}

In this paper published 3 Oct 2018 the authors train a depth estimating network on only stereo image pairs. After the depth predicting network has been trained the pose network from section \ref{sec:relwork:unego} is trained, using results from the already trained depth predicting network. This means that the depth and pose networks are not trained with a joint loss function but are instead trained separately.

The main contributions from this paper is the proposal to use supixel-convolution. The method additionally uses differentiable flip-augmentation to remove edge artifacats on the left and right edges of the depth map seen in previous work using stereo image pairs during training.

To handle occluded pixels between the left and right images an occulsion regularization loss term is added to encourage background depths (low disparaty).

\section{3D Packing for Self-Supervised Monocular Depth Estimation \cite{packnet}}
The authors present PacknetSfM in their paper on 6 Dec 2019. The main contribution is a new network architecture with packing and unpacking blocks replacing down and up sampling. The new packing blocks use space to depth transformations and 3d convolutions. The authors claim that the new packing and unpacking blocks are better at perserving resolution than standard down and upsampling. As the method is not adopted in later work, it is unclear if the new packing and unpacking approach is actually any good.

The second contribution is a loss on the camera velocity that makes it possible for the monocular depth estimation to be metrically accurate. The velocity of the car is assumed to be known in the training dataset.

The third contribution is a mask on pixels that do not change between frames. These pixels are assumed to have no ego motion. Stationary pixels can occur for example if the car dashboard is visible in a frame, or other vehicles are moving at a similar speed nearby.

\section{Digging Into Self-Supervised Monocular Depth Estimation \cite{monodepth2}}
The authors present MonoDepth2 on 17 Aug 2019. They propose mainly two contributions.

Firstly, instead of taking the average of the reprojection errors from all source frames given a target frame they use the minimum. This makes it so that if a feature is occluded in one source image but not in the other the errors will not be averaged together but instead the error from the source frame that is not occluded will be used.

Secondly, instead of calculating the loss for each depth scale in the decoder, all the depth maps are upsampled to the original target image size when computing the loss. This way a single pixel in the low resolution layer of the decoder will predict the depth of a patch of pixels in the originally sized input image.

\section{Self-Supervised 3D Keypoint Learning for Ego-motion Estimation \cite{keypointdepth}}

This paper from 7 Dec 2019 combines the work of the previously discussed papers and also adds keypoint learning from SuperPoint\cite{superpoint} and its successor UnsuperPoint\cite{unsuperpoint} into the pipeline.
The researchers train the depth, keypoint and pose estimating networks jointly, making them benifit from each other and achieve state of the art results. However, the step that finds possible outlier keypoints and also estimates an initial guess for the camera pose, is not differentiable.

\section{Unsupervised Learning of Consensus Maximization for 3D Vision Problems}

