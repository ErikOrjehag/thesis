@article{transportation,
	author = {Kim, Tschangho},
	year = {2018},
	month = {01},
	pages = {137-150},
	title = {Automated Autonomous Vehicles: Prospects and Impacts on Society},
	volume = {08},
	journal = {Journal of Transportation Technologies},
	doi = {10.4236/jtts.2018.83008}
}

@article{sociology,
	author = {Bissell, David and Birtchnell, Thomas and Elliott, Anthony and Hsu, Eric},
	year = {2018},
	month = {12},
	pages = {001139211881674},
	title = {Autonomous automobilities: The social impacts of driverless vehicles},
	volume = {68},
	journal = {Current Sociology},
	doi = {10.1177/0011392118816743}
}

@inbook{dlbook326,
	title={Deep Learning},
	author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
	publisher={MIT Press},
	note={\url{http://www.deeplearningbook.org}},
	year={2016},
	pages={326-327}
}

@inbook{dlbook164,
	title={Deep Learning},
	author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
	publisher={MIT Press},
	note={\url{http://www.deeplearningbook.org}},
	year={2016},
	pages={164-165}
}

@inbook{dlbook273,
	title={Deep Learning},
	author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
	publisher={MIT Press},
	note={\url{http://www.deeplearningbook.org}},
	year={2016},
	pages={273}
}

@inbook{dlbook80,
	title={Deep Learning},
	author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
	publisher={MIT Press},
	note={\url{http://www.deeplearningbook.org}},
	year={2016},
	pages={80-82}
}



@inproceedings{lidarvscamera,
	author = {Wang, Yan and Chao, Wei-Lun and Garg, Divyansh and Hariharan, Bharath and Campbell, Mark and Weinberger, Kilian},
	year = {2019},
	month = {06},
	pages = {8437-8445},
	title = {Pseudo-LiDAR From Visual Depth Estimation: Bridging the Gap in 3D Object Detection for Autonomous Driving},
	doi = {10.1109/CVPR.2019.00864}
}

@ARTICLE{orbslam,
	author={R. {Mur-Artal} and J. M. M. {Montiel} and J. D. {Tardós}},
	journal={IEEE Transactions on Robotics}, 
	title={ORB-SLAM: A Versatile and Accurate Monocular SLAM System}, 
	year={2015},
	volume={31},
	number={5},
	pages={1147-1163},
	doi={10.1109/TRO.2015.2463671}}

@article{ssim,
	author={ {Zhou Wang} and A. C. {Bovik} and H. R. {Sheikh} and E. P. {Simoncelli}},
	journal={IEEE Transactions on Image Processing}, 
	title={Image quality assessment: from error visibility to structural similarity}, 
	year={2004},
	volume={13},
	number={4},
	pages={600-612}
}

@article{spatialtransformernetworks,
	author = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and Kavukcuoglu, Koray},
	year = {2015},
	month = {06},
	pages = {},
	title = {Spatial Transformer Networks},
	journal = {Advances in Neural Information Processing Systems 28 (NIPS 2015)}
}

@misc{lyft2019,
	title = {Lyft Level 5 AV Dataset 2019},
	author = {Kesten, R. and Usman, M. and Houston, J. and Pandya, T. and Nadhamuni, K. and Ferreira, A. and Yuan, M. and Low, B. and Jain, A. and Ondruska, P. and Omari, S. and Shah, S. and Kulkarni, A. and Kazakova, A. and Tao, C. and Platinsky, L. and Jiang, W. and Shet, V.},
	year = {2019},
	howpublished = {url{https://level5.lyft.com/dataset/}}
}

@article{kitti,
	author = {Geiger, Andreas and Lenz, P and Stiller, Christoph and Urtasun, Raquel},
	year = {2013},
	month = {09},
	pages = {1231-1237},
	title = {Vision meets robotics: the KITTI dataset},
	volume = {32},
	journal = {The International Journal of Robotics Research},
	doi = {10.1177/0278364913491297}
}

@article{resnet,
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2015},
	month = {12},
	pages = {},
	title = {Deep Residual Learning for Image Recognition},
	volume = {7}
}

@InProceedings{dispnet,
	author       = "N.Mayer and E.Ilg and P.H{\"a}usser and P.Fischer and D.Cremers and A.Dosovitskiy and T.Brox",
	title        = "A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation",
	booktitle    = "IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)",
	month        = " ",
	year         = "2016",
	note         = "arXiv:1512.02134",
	url          = "http://lmb.informatik.uni-freiburg.de/Publications/2016/MIFDB16"
}

@article{unsuperpoint,
	author    = {Peter Hviid Christiansen and
	Mikkel Fly Kragh and
	Yury Brodskiy and
	Henrik Karstoft},
	title     = {UnsuperPoint: End-to-end Unsupervised Interest Point Detector and
	Descriptor},
	journal   = {CoRR},
	volume    = {abs/1907.04011},
	year      = {2019},
	url       = {http://arxiv.org/abs/1907.04011},
	archivePrefix = {arXiv},
	eprint    = {1907.04011},
	timestamp = {Wed, 17 Jul 2019 10:27:36 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1907-04011.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{superpoint,
	author = {DeTone, Daniel and Malisiewicz, Tomasz and Rabinovich, Andrew},
	year = {2017},
	month = {12},
	pages = {},
	title = {SuperPoint: Self-Supervised Interest Point Detection and Description}
}

@unknown{keypointdepth,
	author = {Tang, Jiexiong and Ambrus, Rares and Guizilini, Vitor and Pillai, Sudeep and Kim, Hanme and Gaidon, Adrien},
	year = {2019},
	month = {12},
	pages = {},
	title = {Self-Supervised 3D Keypoint Learning for Ego-motion Estimation}
}

@unknown{monodepth2,
	author = {Godard, Clément and Aodha, Oisin and Brostow, Gabriel},
	year = {2018},
	month = {06},
	pages = {},
	title = {Digging Into Self-Supervised Monocular Depth Estimation}
}

@unknown{packnet,
	author = {Guizilini, Vitor and Ambrus, Rares and Pillai, Sudeep and Gaidon, Adrien},
	year = {2019},
	month = {05},
	pages = {},
	title = {PackNet-SfM: 3D Packing for Self-Supervised Monocular Depth Estimation}
}

@inproceedings{superdepth,
	author = {Pillai, Sudeep and Ambrus, Rares and Gaidon, Adrien},
	year = {2019},
	month = {05},
	pages = {9250-9256},
	title = {SuperDepth: Self-Supervised, Super-Resolved Monocular Depth Estimation},
	doi = {10.1109/ICRA.2019.8793621}
}

@article{sfmlearner,
	author = {Zhou, Tinghui and Brown, Matthew and Snavely, Noah and Lowe, David},
	year = {2017},
	month = {04},
	pages = {},
	title = {Unsupervised Learning of Depth and Ego-Motion from Video}
}

@inproceedings{leftright,
	author = {Godard, Clement and Aodha, Oisin and Brostow, Gabriel},
	year = {2017},
	month = {07},
	pages = {},
	title = {Unsupervised Monocular Depth Estimation with Left-Right Consistency},
	doi = {10.1109/CVPR.2017.699}
}


@article{PointNet,
	author    = {Charles Ruizhongtai Qi and
	Hao Su and
	Kaichun Mo and
	Leonidas J. Guibas},
	title     = {PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation},
	journal   = {CoRR},
	volume    = {abs/1612.00593},
	year      = {2016},
	annote    = {http://arxiv.org/abs/1612.00593},
	archivePrefix = {arXiv},
	eprint    = {1612.00593},
	timestamp = {Mon, 13 Aug 2018 16:48:27 +0200},
	biburl    = {https://dblp.org/rec/bib/journals/corr/QiSMG16},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{MatchNet,
	author={ {Xufeng Han} and T. {Leung} and Y. {Jia} and R. {Sukthankar} and A. C. {Berg}},
	booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	title={MatchNet: Unifying feature and metric learning for patch-based matching},
	year={2015},
	volume={},
	number={},
	pages={3279-3286},
	abstract={Motivated by recent successes on learning feature representations and on learning feature comparison functions, we propose a unified approach to combining both for training a patch matching system. Our system, dubbed Match-Net, consists of a deep convolutional network that extracts features from patches and a network of three fully connected layers that computes a similarity between the extracted features. To ensure experimental repeatability, we train MatchNet on standard datasets and employ an input sampler to augment the training set with synthetic exemplar pairs that reduce overfitting. Once trained, we achieve better computational efficiency during matching by disassembling MatchNet and separately applying the feature computation and similarity networks in two sequential stages. We perform a comprehensive set of experiments on standard datasets to carefully study the contributions of each aspect of MatchNet, with direct comparisons to established methods. Our results confirm that our unified approach improves accuracy over previous state-of-the-art results on patch matching datasets, while reducing the storage requirement for descriptors. We make pre-trained MatchNet publicly available.},
	keywords={feature extraction;image matching;image representation;learning (artificial intelligence);MatchNet;feature learning;metric learning;patch-based matching;feature representations;patch matching system;deep convolutional network;feature extraction;Measurement;Training;Poles and towers;Feature extraction;Reservoirs;Standards;Robustness},
	doi={10.1109/CVPR.2015.7298948},
	ISSN={1063-6919},
	month={June}
}

@InProceedings{LIFT,
	author="Yi, Kwang Moo
	and Trulls, Eduard
	and Lepetit, Vincent
	and Fua, Pascal",
	editor="Leibe, Bastian
	and Matas, Jiri
	and Sebe, Nicu
	and Welling, Max",
	title="LIFT: Learned Invariant Feature Transform",
	booktitle="Computer Vision -- ECCV 2016",
	year="2016",
	publisher="Springer International Publishing",
	address="Cham",
	pages="467--483",
	abstract="We introduce a novel Deep Network architecture that implements the full feature point handling pipeline, that is, detection, orientation estimation, and feature description. While previous works have successfully tackled each one of these problems individually, we show how to learn to do all three in a unified manner while preserving end-to-end differentiability. We then demonstrate that our Deep pipeline outperforms state-of-the-art methods on a number of benchmark datasets, without the need of retraining.",
	isbn="978-3-319-46466-4",
	annote={http://arxiv.org/abs/1603.09114}
}

@InProceedings{ORB,
	author={E. {Rublee} and V. {Rabaud} and K. {Konolige} and G. {Bradski}},
	booktitle={2011 International Conference on Computer Vision},
	title={ORB: An efficient alternative to SIFT or SURF},
	year={2011},
	pages={2564-2571},
	abstract={Feature matching is at the base of many computer vision problems, such as object recognition or structure from motion. Current methods rely on costly descriptors for detection and matching. In this paper, we propose a very fast binary descriptor based on BRIEF, called ORB, which is rotation invariant and resistant to noise. We demonstrate through experiments how ORB is at two orders of magnitude faster than SIFT, while performing as well in many situations. The efficiency is tested on several real-world applications, including object detection and patch-tracking on a smart phone.},
	keywords={computer vision;image matching;object detection;object recognition;tracking;transforms;ORB;SIFT;SURF;feature matching;computer vision;object recognition;binary descriptor;BRIEF;noise resistance;object detection;patch-tracking;smart phone;Boats},
	doi={10.1109/ICCV.2011.6126544},
	ISSN={1550-5499},
	month={Nov}
}

@Article{SIFT,
	author="Lowe, David G.",
	title="Distinctive Image Features from Scale-Invariant Keypoints",
	journal="International Journal of Computer Vision",
	year="2004",
	month="Nov",
	day="01",
	volume="60",
	number="2",
	pages="91--110",
	abstract="This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.",
	issn="1573-1405",
	doi="10.1023/B:VISI.0000029664.99615.94",
	annote={https://doi.org/10.1023/B:VISI.0000029664.99615.94}
} 

@article{RANSAC,
	author = {Fischler, Martin A. and Bolles, Robert C.},
	title = {Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography},
	journal = {Commun. ACM},
	issue_date = {June 1981},
	volume = {24},
	number = {6},
	month = jun,
	year = {1981},
	issn = {0001-0782},
	pages = {381--395},
	numpages = {15},
	doi = {10.1145/358669.358692},
	acmid = {358692},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {automated cartography, camera calibration, image matching, location determination, model fitting, scene analysis},
	annote = {http://doi.acm.org/10.1145/358669.358692}
}

@InProceedings{DeepTAM,
	author       = "H. Zhou and B. Ummenhofer and T. Brox",
	title        = "DeepTAM: Deep Tracking and Mapping",
	booktitle    = "European Conference on Computer Vision (ECCV)",
	month        = " ",
	year         = "2018",
	url          = "http://lmb.informatik.uni-freiburg.de/Publications/2018/ZUB18"
}

@InProceedings{consensus,
	author = {Probst, Thomas and Paudel, Danda Pani and Chhatkuli, Ajad and Gool, Luc Van},
	title = {Unsupervised Learning of Consensus Maximization for 3D Vision Problems},
	booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	month = {June},
	year = {2019}
}